{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N5RuFIhveel"
      },
      "source": [
        "# Homework assignment 1: Markov Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1UaCc1Qveen"
      },
      "source": [
        "## Problem 1\n",
        "Read the article in https://www.geeksforgeeks.org/markov-chains-in-nlp/, and answer the following questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBof0iUZveeo"
      },
      "source": [
        "* (a) What are N-grams of an input sequence?\n",
        "    * According to the article, N-grams are **contiguous sequences of N items from a given input sequence**. In the context of natural language processing (NLP), N could refer to the number of words or characters.\n",
        "    \n",
        "    For instance, in a sentence, a **2-gram** represents all possible **pairs** of consecutive **bold text** words and a **3-gram** represents all possible **triplets** of **consecutive words**. N-grams help in probabilistic language models such as the Markov chain model.\n",
        "\n",
        "* (b) How to determine the probability of each element in the transition matrix?\n",
        "    * To determine the probability of each element in the transition matrix, we have to analyze the collection of texts to determine the probabilities of transitioning from one word to another.  \n",
        "    \n",
        "    1. Let **each word** or character be a **state**\n",
        "    2. **Count the number of transitions from one state** (or word, character, etc.) **to another**\n",
        "    3. **Divide the count from** (2) by the **total** number of transitions from the same starting state\n",
        "\n",
        "* (c) If you want to increase the variety of the sequence generation (every time the outputs are different), what kinds of properties the training texts should be?\n",
        "    * To increase the variety of the sequence generation, we can:\n",
        "    1. **Vocabulary**-wise: Use a **larger**, **more varied** vocabulary to have wider options of potential next states in a sequence, making generated sequences less predictable. One way to do this is to use synonyms for words with the same meaning.\n",
        "    2. **Sentence Structure**-wise: How the sentences are structured should be more **complex** and **longer** so the model can learn from more possible transitions between states (words or characters)\n",
        "    3. **Word Combinations**-wise: The training text **should not overly repeat certain patterns**, as it could lead to repetitive outputs. If certain words or patterns are repeated too often, the model will tend to be biased toward generating those specific patterns (assign a higher probability to those transitions), even if the sentence structure is varied.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW3fVQRdveeo"
      },
      "source": [
        "## Problem 2\n",
        "Try the following codes, and answer questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GTJO0ntfEuUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e564123-213e-421c-ed13-116c893065fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "# Install the required packages\n",
        "!pip install nltk\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3rvAjDpaveep"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk import ngrams\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Union, Dict\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fuUPhTVWveeq"
      },
      "outputs": [],
      "source": [
        "def words_to_index(words: List[str], base: int, unique_words: Dict[str, int]) -> int:\n",
        "    \"\"\"\n",
        "        Convert a list of words to an index representation.\n",
        "\n",
        "        Args:\n",
        "            words (List[str]): The list of words to be converted.\n",
        "            base (int): The base value used for conversion.\n",
        "            unique_words (Dict[str, int]): A dictionary mapping unique words to their corresponding indices.\n",
        "\n",
        "        Returns:\n",
        "            - int: The index representation of the given list of words.\n",
        "\n",
        "        Example:\n",
        "            >>> words_to_index([\"the\", \"quick\", \"brown\", \"fox\"], 10, {\"the\": 0, \"quick\": 1, \"brown\": 2, \"fox\": 3})\n",
        "            123\n",
        "    \"\"\"\n",
        "    length = len(words)\n",
        "    numbers = [unique_words[word] for word in words]\n",
        "    return sum([num * (base ** (length - 1 - idx)) for idx, num in enumerate(numbers)])\n",
        "\n",
        "def index_to_words(index: int, base: int, length: int, unique_words: Dict[str, int]):\n",
        "    \"\"\"\n",
        "        Converts an index to a list of words based on a given base and unique words.\n",
        "\n",
        "        Args:\n",
        "            index (int): The index to convert.\n",
        "            base (int): The base used for conversion.\n",
        "            length (int): The length of the resulting list.\n",
        "            unique_words (Dict[str, int]): A dictionary mapping unique words to their corresponding indices.\n",
        "\n",
        "        Returns:\n",
        "            List[str]: A list of words corresponding to the given index.\n",
        "\n",
        "        Examples:\n",
        "            >>> unique_words = {'apple': 0, 'banana': 1, 'cherry': 2}\n",
        "            >>> index_to_words(5, 3, 2, unique_words)\n",
        "            ['banana', 'cherry'] (Because 5 = 1 * 3^1 + 2 * 2^0)\n",
        "\n",
        "            >>> unique_words = {'red': 0, 'green': 1, 'blue': 2}\n",
        "            >>> index_to_words(2, 3, 1, unique_words)\n",
        "            ['blue'] (Because 2 = 2 * 3^0)\n",
        "    \"\"\"\n",
        "    numbers = []\n",
        "\n",
        "    unique_words_list = list(unique_words.keys())\n",
        "    for pow in range(length - 1, -1, -1):\n",
        "        numbers.append(index // (base ** pow))\n",
        "        index -= numbers[-1] * (base ** pow)\n",
        "\n",
        "    return [unique_words_list[num] for num in numbers]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q89RN21veer"
      },
      "source": [
        "### Step 1: Remove some unnecessary characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-bLwYWvDveer"
      },
      "outputs": [],
      "source": [
        "def remove_unnecessary_characters(text: str):\n",
        "    \"\"\"\n",
        "        Removes unnecessary characters from the given text and converts it to lowercase.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text to be processed.\n",
        "        Returns:\n",
        "            str: The processed text with unnecessary characters removed and converted to lowercase.\n",
        "    \"\"\"\n",
        "    # Remove unnecessary characters\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDPRDP_Dveer"
      },
      "source": [
        "### Step 2: Generate N-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_XqvOwjOvees"
      },
      "outputs": [],
      "source": [
        "def generate_n_grams(text: str, n: int):\n",
        "    \"\"\"\n",
        "        Generate n-grams from the given text.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text from which n-grams will be generated.\n",
        "            n (int): The number of consecutive words in each n-gram.\n",
        "        Returns:\n",
        "            List[Tuple[str]]: A list of tuples representing the generated n-grams.\n",
        "    \"\"\"\n",
        "    # [TODO] Generate n-grams, using the ngrams function from the nltk library\n",
        "    n_grams = ngrams(text.split(), n)\n",
        "\n",
        "    # Convert to list of tuples\n",
        "    n_grams = list(n_grams)\n",
        "\n",
        "    return n_grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLrCGQuyvees"
      },
      "source": [
        "### Step 3: Compute Transition Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ij_Pxsn6vees"
      },
      "outputs": [],
      "source": [
        "def compute_transition_matrix(n_grams: List[Tuple[str]]):\n",
        "    \"\"\"\n",
        "        Computes the transition matrix and unique states for a given list of n-grams.\n",
        "\n",
        "        Args:\n",
        "            n_grams (List[Tuple[str]]): A list of n-grams, where each n-gram is a tuple of strings.\n",
        "        Returns:\n",
        "            np.ndarray: transition matrix.\n",
        "            Dict[str, int]: A dictionary mapping unique words to their corresponding indices.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the value of n\n",
        "    n = len(n_grams[0])\n",
        "\n",
        "    ## Step 3.1: Collect all possible words (label words with indices)\n",
        "\n",
        "    unique_words = {}\n",
        "    unique_words_count = 0\n",
        "\n",
        "    # A helper function to add a word to the unique_words dictionary, if it is not already present\n",
        "    def _add_to_unique_word(word: str):\n",
        "        # We need to increase the counter outside this function\n",
        "        nonlocal unique_words_count\n",
        "\n",
        "        # Check if the state is already in the unique_states dictionary\n",
        "        if unique_words.get(word, None) is None:\n",
        "            unique_words[word] = unique_words_count\n",
        "            unique_words_count += 1\n",
        "\n",
        "    # Iterate over all n-grams to collect all possible words\n",
        "    for n_gram in n_grams:\n",
        "        for word in n_gram:\n",
        "            # [TODO] Add the word to the unique_words dictionary\n",
        "            # hint: use the _add_to_unique_word function\n",
        "            _add_to_unique_word(word)\n",
        "            continue\n",
        "\n",
        "\n",
        "    ## Step 3.2: Compute the transition matrix\n",
        "\n",
        "    # In this part, we use `words_to_index` and `index_to_words` functions to convert between state name (words) and indices\n",
        "\n",
        "    # Example:\n",
        "    # If we have 3 unique words: 0 for \"apple\", 1 for \"banana\", 2 for \"cherry\", and n = 3,\n",
        "    # then the state names are \"apple, apple\", \"apple, banana\", \"apple, cherry\", \"banana, apple\", \"banana, banana\", and so on.\n",
        "    # In this case, the mapping from states to indices would be:\n",
        "    # \"(apple, apple)\" <-> 0, \"(apple, banana)\" <-> 1, \"(apple, cherry)\" <-> 2, \"(banana, apple)\" <-> 3, and so on.\n",
        "\n",
        "    # [TODO] Compute the number of states in Markov chain. Number of states is the number of unique words to the power of n-1\n",
        "    unique_states_count = unique_words_count ** (n - 1)\n",
        "\n",
        "    # [TODO] Create transition matrix, filled with zeros (the size of the transition matrix is # states x # states)\n",
        "    transition_matrix = np.zeros((unique_states_count, unique_states_count))\n",
        "\n",
        "    # Count the number of transitions from each state to another state\n",
        "    for n_gram in n_grams:\n",
        "        # [TODO] Find the indices of the states (from and to) (i.e. the first n-1 words and the last n-1 words)\n",
        "        # hint: use words_to_index function\n",
        "        state_from_index = words_to_index(n_gram[:-1], unique_words_count, unique_words)\n",
        "        state_to_index = words_to_index(n_gram[1:], unique_words_count, unique_words)\n",
        "\n",
        "        # Increment the count of the transition from state_from to state_to\n",
        "        transition_matrix[state_from_index][state_to_index] += 1\n",
        "\n",
        "    # Special case:\n",
        "    # if the sum of number of transition of a state is zero, which means it has no information about the next state,\n",
        "    # we set the probabilities of transitioning to all possible next states to 1 / # words\n",
        "\n",
        "    # Check if any row sums to zero\n",
        "    row_sums_is_zero = transition_matrix.sum(axis=1) == 0\n",
        "\n",
        "    # Iterate over rows to find zero-sum rows\n",
        "    first_word = list(unique_words.keys())[0]\n",
        "    for idx, is_zero in enumerate(row_sums_is_zero):\n",
        "        # If the row sum is zero\n",
        "        if is_zero:\n",
        "            # Convert the index to state representation\n",
        "            state_name = index_to_words(idx, unique_words_count, n-1, unique_words)\n",
        "\n",
        "            # Calculate the range of indices for the next posiible states\n",
        "            possible_states_start_index = words_to_index(state_name[1:] + [first_word, ], unique_words_count, unique_words)\n",
        "            possible_states_end_index = possible_states_start_index + unique_words_count\n",
        "\n",
        "            # Set the number of transitions to 1 for the range\n",
        "            transition_matrix[idx, possible_states_start_index : possible_states_end_index] = 1\n",
        "\n",
        "    ## Step 3.3: Normalize the transition matrix\n",
        "\n",
        "    # The following steps is to normalize transition matrix so that the sum of each row is 1\n",
        "    # (The reason is: By the definition of a Markov chain, the sum of each row should be 1)\n",
        "\n",
        "    # [TODO] Compute the sum of each row\n",
        "    # hint: use np.sum function and check if the axis parameter is set correctly\n",
        "    row_sums = np.sum(transition_matrix, axis=1, keepdims=True)\n",
        "\n",
        "    # [TODO] Divide the transition matrix by the sum of each row to make the sum of each row equals to 1\n",
        "    transition_matrix = np.divide(transition_matrix, row_sums, where=row_sums != 0)\n",
        "\n",
        "    return transition_matrix, unique_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLiaaGcpveet"
      },
      "source": [
        "### Step 4: Generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Bh7NXbk4veet"
      },
      "outputs": [],
      "source": [
        "def generate(unique_words: Dict[str, int], transition_matrix: np.ndarray, start_from: Union[str, List[str]], n: int, length: int=10):\n",
        "    \"\"\"\n",
        "        Generate text using a Markov chain model.\n",
        "\n",
        "        Args:\n",
        "            unique_states (Dict[str, int]): A dictionary mapping unique words to their corresponding indices.\n",
        "            transition_matrix (np.ndarray): A numpy array representing the transition probabilities between states.\n",
        "            start_from (Union[str, List[str]]): The starting state(s) for text generation.\n",
        "            n (int): The size of the grams.\n",
        "            length (int, optional): The length of the generated text. Defaults to 10.\n",
        "\n",
        "        Returns:\n",
        "            The generated text.\n",
        "    \"\"\"\n",
        "    # Generate text\n",
        "    generated_words = start_from.copy() if type(start_from) is list else [start_from]\n",
        "\n",
        "    # Assert if the number of start words does not equal to n-1\n",
        "    assert len(generated_words) >= n-1, \"The number of start words should be greater than or equals to n-1 ({})\".format(n-1)\n",
        "\n",
        "    # Get the number of unique words\n",
        "    unique_words_count = len(unique_words)\n",
        "\n",
        "    # [TODO] Get the number of unique states\n",
        "    # hint: check step 3.2 in the compute_transition_matrix function\n",
        "    unique_states_count = unique_words_count ** (n - 1)\n",
        "\n",
        "    # Generate the next words\n",
        "    for _ in range(length):\n",
        "        # [TODO] Get index of current states\n",
        "        # hint: The current states (current words) is the last n-1 words in the generated text\n",
        "        # hint: use words_to_index function\n",
        "        current_words_index = words_to_index(generated_words[-(n-1):], unique_words_count, unique_words)\n",
        "\n",
        "        # [TODO] Get probability distribution for next state, using the information in the transition matrix\n",
        "        probabilities = transition_matrix[current_words_index]\n",
        "\n",
        "        # Select next word based on probabilities, using np.random.choice function\n",
        "        next_words_index = np.random.choice(unique_states_count, p=probabilities)\n",
        "\n",
        "        # [TODO] Decode the index and get the last word\n",
        "        # hint: use index_to_words function\n",
        "        next_word = index_to_words(next_words_index, unique_words_count, n-1, unique_words)[-1]\n",
        "\n",
        "        # Add next word to generated text\n",
        "        generated_words.append(next_word)\n",
        "\n",
        "    # return generated string\n",
        "    return ' '.join(generated_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [TODO] Change the text below and try different values of n\n",
        "text = \"Linear Algebra presents a real challenge. While I anticipated the complexity of the topics, I didn’t expect it to be this difficult to grasp. The concepts, though powerful and foundational, often feel overwhelming. Despite understanding their importance, translating them into something intuitive seems much harder than I imagined.\"\n",
        "n = 4\n",
        "\n",
        "# Process the text and generate the transition matrix\n",
        "text = remove_unnecessary_characters(text)\n",
        "n_grams = generate_n_grams(text, n)\n",
        "transition_matrix, unique_words = compute_transition_matrix(n_grams)"
      ],
      "metadata": {
        "id": "VQfPzpaYnEWZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4MAL08Egveeu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "937788c8-6d70-470f-fc9c-3cd2adde48c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== The indices for unique states are: =====\n",
            "linear,linear,linear: 0\n",
            "linear,linear,algebra: 1\n",
            "linear,linear,presents: 2\n",
            "linear,linear,a: 3\n",
            "linear,linear,real: 4\n",
            "linear,linear,challenge: 5\n",
            "linear,linear,while: 6\n",
            "linear,linear,i: 7\n",
            "linear,linear,anticipated: 8\n",
            "linear,linear,the: 9\n",
            "linear,linear,complexity: 10\n",
            "linear,linear,of: 11\n",
            "linear,linear,topics: 12\n",
            "linear,linear,didnt: 13\n",
            "linear,linear,expect: 14\n",
            "linear,linear,it: 15\n",
            "linear,linear,to: 16\n",
            "linear,linear,be: 17\n",
            "linear,linear,this: 18\n",
            "linear,linear,difficult: 19\n",
            "...\n",
            "\n",
            "===== The transition matrix is (Shape of trasition matrix: (79507, 79507)): =====\n",
            "[[0.02325581 0.02325581 0.02325581 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.02325581 0.02325581 0.02325581]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the transition matrix and unique states for obsevation\n",
        "print(\"===== The indices for unique states are: =====\")\n",
        "unique_words_count = len(unique_words)\n",
        "for word_name in list(product(*[unique_words for _ in range(n-1)]))[:20]:\n",
        "    print(f\"{','.join(word_name):10s}: {words_to_index(word_name, unique_words_count, unique_words)}\")\n",
        "print(\"...\", end=\"\\n\\n\")\n",
        "\n",
        "print(\"===== The transition matrix is (Shape of trasition matrix: {}): =====\".format(transition_matrix.shape))\n",
        "print(transition_matrix)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rnbUL1itveev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d41a7e78-f66f-4651-d78c-1fccf0b29ddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#1 (length=15): a real challenge while i anticipated the complexity of the topics i didnt expect it to be this\n",
            "#2 (length=25): linear algebra presents a real challenge while i anticipated the complexity of the topics i didnt expect it to be this difficult to grasp the concepts though powerful and\n",
            "#3 (length=40): while i anticipated the complexity of the topics i didnt expect it to be this difficult to grasp the concepts though powerful and foundational often feel overwhelming despite understanding their importance translating them into something intuitive seems much harder than i imagined the\n"
          ]
        }
      ],
      "source": [
        "# [TODO] Write down 3 or more initial words and length of generated text to start the text generation\n",
        "experiments = [\n",
        "    # Each entry is (Initial words, the length of the generated text)\n",
        "    ('a real challenge', 15), ('linear algebra presents a', 25), ('while i anticipated', 40)\n",
        "]\n",
        "\n",
        "for idx, (start_from, length) in enumerate(experiments, 1):\n",
        "    start_from = start_from.split(\" \")\n",
        "\n",
        "    # Generate text using the transition matrix\n",
        "    generated_text = generate(unique_words, transition_matrix, start_from, n, length=length)\n",
        "\n",
        "    # Print out the generated text\n",
        "    print(\"#{} (length={}): {}\".format(idx, length, generated_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrK8ourT6aE2"
      },
      "source": [
        "### Answer the following questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrUZ91rpvcPt"
      },
      "source": [
        "* (a) Write a new text of at least 15 words as the input.\n",
        "    * Linear Algebra presents a real challenge. While I anticipated the complexity of the topics, I didn’t expect it to be this difficult to grasp. The concepts, though foundational, often feel overwhelming.\n",
        "\n",
        "* (b) Run the program 3 times with different output length and different initial words. Show the outputs.\n",
        "    \n",
        "    When N = 4\n",
        "    * **First run:**\n",
        "    \n",
        "    #1 (length=10): i didnt expect it to be this difficult to grasp the concepts though\n",
        "    \n",
        "    #2 (length=15): i anticipated the complexity of the topics i didnt expect it to be this difficult to grasp the\n",
        "    \n",
        "    #3 (length=30): linear algebra presents a real challenge while i anticipated the complexity of the topics i didnt expect it to be this difficult to grasp the concepts though foundational often feel overwhelming difficult i\n",
        "\n",
        "    * **Second run:**\n",
        "    \n",
        "    #1 (length=5): the concepts though foundational often feel overwhelming algebra\n",
        "    \n",
        "    #2 (length=10): the complexity of the topics i didnt expect it to be this difficult to\n",
        "    \n",
        "    #3 (length=20): linear algebra presents a real challenge while i anticipated the complexity of the topics i didnt expect it to be this difficult to grasp\n",
        "\n",
        "    * **Third run:**\n",
        "\n",
        "    #1 (length=15): a real challenge while i anticipated the complexity of the topics i didnt expect it to be this\n",
        "    \n",
        "    #2 (length=25): linear algebra presents a real challenge while i anticipated the complexity of the topics i didnt expect it to be this difficult to grasp the concepts though foundational often\n",
        "    \n",
        "    #3 (length=40): while i anticipated the complexity of the topics i didnt expect it to be this difficult to grasp the concepts though foundational often feel overwhelming this concepts of concepts foundational complexity i topics a often linear the often grasp though grasp feel be\n",
        "\n",
        "* (c) Try different N of N-grams. How the N influences the output sequence?\n",
        "    * When I tried **N = 3**:\n",
        "\n",
        "    #1 (length=15): a real challenge while i anticipated the complexity of the topics i didnt expect it to be this\n",
        "    \n",
        "    #2 (length=25): linear algebra presents a real challenge while i anticipated the complexity of the topics i didnt expect it to be this difficult to grasp the concepts though foundational often\n",
        "    \n",
        "    #3 (length=40): while i anticipated the complexity of the topics i didnt expect it to be this difficult to grasp the concepts though foundational often feel overwhelming difficult anticipated of this overwhelming difficult feel often though real though complexity challenge though challenge algebra often a\n",
        "    \n",
        "    * However, when I tried **N = 2**：\n",
        "\n",
        "    #1 (length=15): a real challenge while i anticipated the topics i didnt expect it to grasp the topics i anticipated\n",
        "    \n",
        "    #2 (length=25): linear algebra presents a real challenge while i anticipated the concepts though foundational often feel overwhelming real challenge while i didnt expect it to grasp the concepts though foundational\n",
        "    \n",
        "    #3 (length=40): while i anticipated the topics i anticipated the concepts though foundational often feel overwhelming concepts though foundational often feel overwhelming algebra presents a real challenge while i didnt expect it to be this difficult to be this difficult to be this difficult to\n",
        "\n",
        "    * Conclusion: **Larger** n-grams produce **more coherent but repetitive** (due to the small data of training text) sequences, while **smaller** n-grams result in **more diverse but less structured** output.\n",
        "    \n",
        "    So, the larger n-grams perform better when the goal is to generate text that closely resembles the original input, while smaller n-grams makes the output becomes more random, and the structure of the original input is lost\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ej2DuDTveev"
      },
      "source": [
        "## Problem 3\n",
        "The Stationary Distribution of a Markov chain is a distribution of probabilities that remains unchanged after a transition from one state to another."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EGILVlKvcPu"
      },
      "source": [
        "* (a) Ask an LLM (Large Language Model), such as ChatGPT, what are the applications of stationary distribution of a Markov chain. You need to show which prompts are used, and state how you verify the correctness of the results (output by LLMs).\n",
        "\n",
        "    For a better result, I used both ChatGPT and Gemini by using the following two prompts\n",
        "    * First prompt\n",
        "    \n",
        "    Stationary distribution of a Markov chain refers to a distribution of probabilities that remains unchanged after a transition from one state to another, what are the applications of stationary distribution of a Markov chain?\n",
        "    \n",
        "    Answer:\n",
        "    \n",
        "    PageRank Algorithm: Google uses it to rank web pages by their relevance.\n",
        "    \n",
        "    Supply Chain Optimization: Helps model steady-state probabilities for inventory levels.\n",
        "\n",
        "    Economics: Used in market share analysis and modeling steady economic conditions.\n",
        "\n",
        "    Genetics: Tracks gene frequencies in populations over time.\n",
        "\n",
        "    Queueing Systems: Determines long-term behavior in processes like traffic flow or telecommunications.\n",
        "\n",
        "    To verify the correctness of the results, I proceed to my second prompt\n",
        "    * Second prompt\n",
        "    \n",
        "    Show me all the sources where all those statements are based upon\n",
        "\n",
        "    From herunto, for fact-checking, we can search the web for research papers with reliable amount of citations. For instance, I used the paper https://langvillea.people.charleston.edu/MCapps7.pdf which has more than 60 citations to verify some of the statements mentioned by both ChatGPT and Gemini.\n",
        "\n",
        "* (b) Ask an LLM, such as ChatGPT, what numerical method is the most efficient approach to compute the stationary distribution? You need to show which prompts are used, and state how you verify the correctness of the results (output by LLMs)\n",
        "    * First prompt:\n",
        "    \n",
        "    What numerical method is the most efficient approach to compute the stationary distribution of Markov Chain?\n",
        "\n",
        "    Answer:\n",
        "  1. Power Method:\n",
        "  Description: The power iteration method is widely used for finding the stationary distribution. It involves multiplying a probability vector by the transition matrix repeatedly until it converges to a steady state (the stationary distribution).\n",
        "  Efficiency: Simple and often efficient for large, sparse matrices.\n",
        "  Limitations: It can be slow to converge for matrices that have eigenvalues close to 1.\n",
        "  Applications: It is frequently used in algorithms like Google's PageRank​.\n",
        "  2. Eigenvalue Decomposition:\n",
        "  Description: By computing the eigenvector corresponding to the eigenvalue 1 of the transition matrix, one can directly find the stationary distribution.\n",
        "  Efficiency: More computationally intensive than the power method, especially for large matrices.\n",
        "  Limitations: Not suitable for very large matrices due to the computational cost of eigenvalue decomposition.\n",
        "  Applications: Suitable for smaller problems or matrices with known structures​.\n",
        "  3. Gauss-Seidel Iteration:\n",
        "  Description: This method solves for the stationary distribution by iteratively solving systems of linear equations derived from the stationary distribution condition.\n",
        "  Efficiency: Often more efficient than power iteration for certain structured matrices.\n",
        "  Limitations: Requires more memory and can be complex to implement.\n",
        "  Applications: Used in cases where precision and faster convergence are needed compared to the power method\n",
        "  4. Markov Chain Monte Carlo (MCMC):\n",
        "  Description: In cases where the transition matrix is too large to handle directly, MCMC methods can approximate the stationary distribution by sampling from the Markov chain over a long period.\n",
        "  Efficiency: Effective for high-dimensional problems and large datasets.\n",
        "  Limitations: Only provides an approximation, and convergence to the true stationary distribution can be slow if the mixing time is long.\n",
        "  Applications: Widely used in Bayesian statistics, physics simulations, and large-scale machine learning problems\n",
        "\n",
        "  * Second prompt:\n",
        "  Are any of those methods relevant to linear algebra? If so, which one?\n",
        "\n",
        "  Since all those methods are relevant to linear algebra, I proceed to the next prompt since we are dealing with smaller matrices\n",
        "\n",
        "  * Third Prompt: Which one is best for smaller matrices, and which one is best for larger ones for text generation?\n",
        "\n",
        "  The result shows that Power method is best for more complex n-grams like 4-grams or 5grams, while the eigenvalue decomposition is more efficient for small scale problems. Therefore, I asked how it worked and searched the web to research more regarding the power method since I want to test the 4-grams model.\n",
        "\n",
        "\n",
        "* (c) Implement the method suggested by the LLM. Use the transition matrix generated in question 2.a as an input to compute its stationary distribution.\n",
        "    * Please implememt the method `compute_stationary_distribution` below\n",
        "* (d) The theory of probability matrix is given in the textbook 6.8, Eigenvalues/eigenvector of nonegtive matrices. Read textbook 6.8 and ask two questions that you are curious most about it\n",
        "    * How does a probability matrix ensure that the sum of probabilities in each row = 1, and how is this relevant in modelling a Markov chain?\n",
        "    * Why does the property of a stochastic matrix that ensures the rows sums are always 1 guarantee that the system will reach a steady state in a Markov chain?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-VwWF0P1veev"
      },
      "outputs": [],
      "source": [
        "def compute_stationary_distribution(transition_matrix: np.ndarray):\n",
        "    \"\"\"\n",
        "        Compute the stationary distribution of a Markov chain.\n",
        "\n",
        "        Parameters:\n",
        "            transition_matrix (np.ndarray): A numpy array representing the transition probabilities between states.\n",
        "        Returns:\n",
        "            The stationary distribution of the Markov chain.\n",
        "    \"\"\"\n",
        "    # [TODO] Implement the function to compute the stationary distribution of a Markov chain, using the method suggested by the LLM\n",
        "\n",
        "# Initialize a random probability distribution (normalized vector)\n",
        "    num_states = transition_matrix.shape[0]\n",
        "    stationary_distribution = np.ones(num_states) / num_states  # Start with uniform distribution\n",
        "\n",
        "    # [TODO] Implement the power method to compute the stationary distribution\n",
        "    tol = 1e-6  # tolerance for convergence\n",
        "    max_iter = 1000  # maximum number of iterations\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        next_distribution = np.dot(stationary_distribution, transition_matrix)\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.linalg.norm(next_distribution - stationary_distribution) < tol:\n",
        "            break\n",
        "\n",
        "        # Update the current distribution\n",
        "        stationary_distribution = next_distribution\n",
        "\n",
        "    # Normalize the stationary distribution so it sums to 1\n",
        "    stationary_distribution = stationary_distribution / stationary_distribution.sum()\n",
        "    return stationary_distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-owTykzFveev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47218679-2866-49dc-fbc9-ac546b0f0af8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.24e-05 1.24e-05 1.24e-05 ... 1.27e-05 1.27e-05 1.27e-05]\n"
          ]
        }
      ],
      "source": [
        "# Compute the stationary distribution for the transition matrix obtained from the previous problem\n",
        "stationary_distribution = compute_stationary_distribution(transition_matrix)\n",
        "\n",
        "# Convert the stationary distribution to float16 to prevent something like -1.2345678e-16 but it is actually 0\n",
        "print(stationary_distribution.astype(np.float16))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}